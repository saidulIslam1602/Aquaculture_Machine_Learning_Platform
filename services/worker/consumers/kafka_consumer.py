"""
Kafka Consumer Module

Real-time stream processing consumer for fish image classification.
Consumes messages from Kafka topics and processes them asynchronously.

Industry Standards:
    - At-least-once delivery semantics
    - Consumer group for scalability
    - Automatic offset management
    - Error handling and dead letter queue
    - Backpressure handling
    - Graceful shutdown

Architecture:
    - Consumer group for parallel processing
    - Async message processing
    - Batch processing for efficiency
    - Metrics and monitoring
"""

import json
import logging
import signal
import time
from datetime import datetime
from threading import Event
from typing import Any, Dict, Optional

from kafka import KafkaConsumer, KafkaProducer
from kafka.errors import KafkaError

logger = logging.getLogger(__name__)


class FishImageConsumer:
    """
    Kafka Consumer for Fish Image Processing

    Consumes fish images from Kafka topics and processes them in real-time.
    Implements production-grade patterns for reliability and scalability.

    Features:
        - Consumer group for horizontal scaling
        - Automatic offset commits
        - Error handling with DLQ
        - Graceful shutdown
        - Performance metrics
        - Backpressure handling

    Example:
        >>> consumer = FishImageConsumer(
        ...     bootstrap_servers='kafka:9092',
        ...     group_id='fish-processors'
        ... )
        >>> consumer.start()

    Architecture:
        - Subscribes to 'fish-images' topic
        - Processes messages asynchronously
        - Publishes results to 'fish-predictions' topic
        - Failed messages go to 'fish-images-dlq'
    """

    def __init__(
        self,
        bootstrap_servers: str = "kafka:9092",
        group_id: str = "fish-image-processors",
        topics: list = None,
        auto_offset_reset: str = "earliest",
        enable_auto_commit: bool = True,
        max_poll_records: int = 100,
        session_timeout_ms: int = 30000,
        heartbeat_interval_ms: int = 10000,
    ):
        """
        Initialize Kafka Consumer

        Args:
            bootstrap_servers: Kafka broker addresses
            group_id: Consumer group ID for coordination
            topics: List of topics to subscribe to
            auto_offset_reset: Where to start reading ('earliest' or 'latest')
            enable_auto_commit: Auto-commit offsets
            max_poll_records: Max records per poll
            session_timeout_ms: Session timeout for consumer
            heartbeat_interval_ms: Heartbeat interval

        Note:
            Consumer group enables horizontal scaling.
            Multiple consumers in same group share partition load.
        """
        self.bootstrap_servers = bootstrap_servers
        self.group_id = group_id
        self.topics = topics or ["fish-images"]

        # Consumer configuration following best practices
        self.consumer_config = {
            "bootstrap_servers": bootstrap_servers,
            "group_id": group_id,
            "auto_offset_reset": auto_offset_reset,  # Start from beginning or end
            "enable_auto_commit": enable_auto_commit,  # Auto-commit offsets
            "auto_commit_interval_ms": 5000,  # Commit every 5 seconds
            "max_poll_records": max_poll_records,  # Batch size per poll
            "max_poll_interval_ms": 300000,  # 5 minutes max processing time
            "session_timeout_ms": session_timeout_ms,  # Session timeout
            "heartbeat_interval_ms": heartbeat_interval_ms,  # Heartbeat frequency
            "value_deserializer": lambda m: json.loads(
                m.decode("utf-8")
            ),  # JSON deserializer
            "key_deserializer": lambda m: m.decode("utf-8") if m else None,
            "api_version": (2, 5, 0),  # Kafka API version
            "consumer_timeout_ms": 1000,  # Poll timeout
        }

        # Producer for publishing results
        self.producer_config = {
            "bootstrap_servers": bootstrap_servers,
            "value_serializer": lambda m: json.dumps(m).encode("utf-8"),
            "key_serializer": lambda m: m.encode("utf-8") if m else None,
            "acks": "all",  # Wait for all replicas
            "retries": 3,  # Retry on failure
            "max_in_flight_requests_per_connection": 5,
            "compression_type": "gzip",  # Compress messages
        }

        # Initialize consumer and producer
        self.consumer: Optional[KafkaConsumer] = None
        self.producer: Optional[KafkaProducer] = None

        # Shutdown event for graceful termination
        self.shutdown_event = Event()

        # Performance metrics
        self.messages_processed = 0
        self.messages_failed = 0
        self.total_processing_time = 0.0
        self.start_time = None

        # Setup signal handlers for graceful shutdown
        signal.signal(signal.SIGTERM, self._signal_handler)
        signal.signal(signal.SIGINT, self._signal_handler)

        logger.info(
            f"FishImageConsumer initialized: " f"group={group_id}, topics={self.topics}"
        )

    def _signal_handler(self, signum, frame):
        """
        Signal Handler for Graceful Shutdown

        Handles SIGTERM and SIGINT for clean shutdown.

        Args:
            signum: Signal number
            frame: Current stack frame
        """
        logger.info(f"Received signal {signum}, initiating graceful shutdown...")
        self.shutdown_event.set()

    def connect(self) -> None:
        """
        Connect to Kafka

        Establishes connection to Kafka brokers and subscribes to topics.

        Raises:
            KafkaError: If connection fails

        Note:
            Implements connection retry logic automatically.
        """
        try:
            # Create consumer
            self.consumer = KafkaConsumer(**self.consumer_config)

            # Subscribe to topics
            self.consumer.subscribe(self.topics)

            # Create producer for results
            self.producer = KafkaProducer(**self.producer_config)

            logger.info(
                f"Connected to Kafka: {self.bootstrap_servers}, "
                f"subscribed to: {self.topics}"
            )

        except KafkaError as e:
            logger.error(f"Failed to connect to Kafka: {e}")
            raise

    def disconnect(self) -> None:
        """
        Disconnect from Kafka

        Closes consumer and producer connections gracefully.
        Commits any pending offsets before closing.
        """
        if self.consumer:
            try:
                # Commit pending offsets
                self.consumer.commit()
                # Close consumer
                self.consumer.close()
                logger.info("Kafka consumer closed")
            except Exception as e:
                logger.error(f"Error closing consumer: {e}")

        if self.producer:
            try:
                # Flush pending messages
                self.producer.flush()
                # Close producer
                self.producer.close()
                logger.info("Kafka producer closed")
            except Exception as e:
                logger.error(f"Error closing producer: {e}")

    def process_message(self, message: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process Single Message

        Processes fish image message and returns prediction result.
        Calls the ML inference service for actual predictions.

        Args:
            message: Message payload from Kafka

        Returns:
            Dict containing processing result

        Raises:
            Exception: If processing fails
        """
        import base64
        import requests
        from PIL import Image
        import io
        
        start_time = time.time()
        
        try:
            # Extract image data and metadata
            image_data = message.get("image_data")
            image_id = message.get("image_id")
            metadata = message.get("metadata", {})

            if not image_data:
                raise ValueError("No image data provided in message")

            # Decode base64 image data
            try:
                image_bytes = base64.b64decode(image_data)
                image = Image.open(io.BytesIO(image_bytes))
            except Exception as e:
                raise ValueError(f"Invalid image data: {e}")

            # Call ML inference service
            try:
                # Use the inference engine directly
                from services.ml_service.inference.engine import inference_engine
                
                prediction_result = await inference_engine.predict(image)
                
                # Format result for Kafka message
                result = {
                    "image_id": image_id,
                    "species": prediction_result["species"],
                    "confidence": prediction_result["confidence"],
                    "all_probabilities": prediction_result.get("all_probabilities", {}),
                    "model_version": prediction_result.get("model_version", "v1.0.0"),
                    "timestamp": datetime.utcnow().isoformat(),
                    "processing_time_ms": int((time.time() - start_time) * 1000),
                    "metadata": metadata,
                }
                
                logger.info(f"Processed image {image_id}: {result['species']} ({result['confidence']:.3f})")
                return result
                
            except Exception as e:
                logger.error(f"ML inference failed for image {image_id}: {e}")
                # Return error result instead of failing completely
                return {
                    "image_id": image_id,
                    "species": "unknown",
                    "confidence": 0.0,
                    "error": str(e),
                    "timestamp": datetime.utcnow().isoformat(),
                    "processing_time_ms": int((time.time() - start_time) * 1000),
                    "metadata": metadata,
                }
                
        except Exception as e:
            logger.error(f"Message processing failed: {e}")
            return {
                "image_id": message.get("image_id", "unknown"),
                "species": "error",
                "confidence": 0.0,
                "error": str(e),
                "timestamp": datetime.utcnow().isoformat(),
                "processing_time_ms": int((time.time() - start_time) * 1000),
                "metadata": message.get("metadata", {}),
            }

    def publish_result(self, topic: str, key: str, value: Dict[str, Any]) -> None:
        """
        Publish Result to Kafka Topic

        Publishes processing result to output topic.

        Args:
            topic: Target topic name
            key: Message key (for partitioning)
            value: Message value (result dict)

        Note:
            Uses async send with callback for error handling.
        """
        try:
            # Send message asynchronously
            future = self.producer.send(topic, key=key, value=value)

            # Add callback for error handling
            future.add_callback(self._on_send_success)
            future.add_errback(self._on_send_error)

        except Exception as e:
            logger.error(f"Failed to publish result: {e}")
            raise

    def _on_send_success(self, record_metadata):
        """Callback for successful message send"""
        logger.debug(
            f"Message sent successfully: "
            f"topic={record_metadata.topic}, "
            f"partition={record_metadata.partition}, "
            f"offset={record_metadata.offset}"
        )

    def _on_send_error(self, exception):
        """Callback for failed message send"""
        logger.error(f"Failed to send message: {exception}")

    def send_to_dlq(self, message: Dict[str, Any], error: str) -> None:
        """
        Send Failed Message to Dead Letter Queue

        Sends messages that failed processing to DLQ for later analysis.

        Args:
            message: Original message
            error: Error description

        Note:
            DLQ messages include original payload plus error details.
        """
        dlq_message = {
            "original_message": message,
            "error": str(error),
            "timestamp": datetime.utcnow().isoformat(),
            "consumer_group": self.group_id,
        }

        try:
            self.publish_result(
                topic="fish-images-dlq",
                key=message.get("image_id", "unknown"),
                value=dlq_message,
            )
            logger.warning(f"Message sent to DLQ: {message.get('image_id')}")
        except Exception as e:
            logger.error(f"Failed to send to DLQ: {e}")

    def start(self) -> None:
        """
        Start Consumer Loop

        Starts consuming messages from Kafka topics.
        Runs until shutdown signal is received.

        Main Processing Loop:
            1. Poll for messages
            2. Process each message
            3. Publish results
            4. Handle errors
            5. Update metrics

        Note:
            Blocks until shutdown. Run in separate thread if needed.
        """
        self.start_time = time.time()

        try:
            # Connect to Kafka
            self.connect()

            logger.info("Starting consumer loop...")

            # Main processing loop
            while not self.shutdown_event.is_set():
                try:
                    # Poll for messages (batch)
                    message_batch = self.consumer.poll(
                        timeout_ms=1000,
                        max_records=self.consumer_config["max_poll_records"],
                    )

                    # Process each partition's messages
                    for topic_partition, messages in message_batch.items():
                        for message in messages:
                            if self.shutdown_event.is_set():
                                break

                            try:
                                # Process message
                                start_time = time.time()

                                result = self.process_message(message.value)

                                processing_time = time.time() - start_time

                                # Publish result
                                self.publish_result(
                                    topic="fish-predictions",
                                    key=message.key,
                                    value=result,
                                )

                                # Update metrics
                                self.messages_processed += 1
                                self.total_processing_time += processing_time

                                # Log progress periodically
                                if self.messages_processed % 100 == 0:
                                    avg_time = (
                                        self.total_processing_time
                                        / self.messages_processed
                                    )
                                    logger.info(
                                        f"Processed {self.messages_processed} messages, "
                                        f"avg time: {avg_time*1000:.2f}ms"
                                    )

                            except Exception as e:
                                # Handle processing error
                                logger.error(
                                    f"Error processing message: {e}", exc_info=True
                                )
                                self.messages_failed += 1

                                # Send to DLQ
                                self.send_to_dlq(message.value, str(e))

                except Exception as e:
                    logger.error(f"Error in consumer loop: {e}", exc_info=True)
                    time.sleep(1)  # Brief pause before retry

            logger.info("Consumer loop terminated")

        finally:
            # Cleanup
            self.disconnect()
            self._log_final_stats()

    def _log_final_stats(self) -> None:
        """Log Final Statistics"""
        if self.start_time:
            runtime = time.time() - self.start_time
            throughput = self.messages_processed / runtime if runtime > 0 else 0

            logger.info(
                f"Consumer statistics:\n"
                f"  Messages processed: {self.messages_processed}\n"
                f"  Messages failed: {self.messages_failed}\n"
                f"  Runtime: {runtime:.2f}s\n"
                f"  Throughput: {throughput:.2f} msg/s\n"
                f"  Avg processing time: "
                f"{(self.total_processing_time/self.messages_processed*1000):.2f}ms"
                if self.messages_processed > 0
                else "  No messages processed"
            )

    def get_stats(self) -> Dict[str, Any]:
        """
        Get Consumer Statistics

        Returns comprehensive consumer statistics.

        Returns:
            Dict containing performance metrics
        """
        runtime = time.time() - self.start_time if self.start_time else 0
        throughput = self.messages_processed / runtime if runtime > 0 else 0
        avg_time = (
            self.total_processing_time / self.messages_processed
            if self.messages_processed > 0
            else 0
        )

        return {
            "messages_processed": self.messages_processed,
            "messages_failed": self.messages_failed,
            "runtime_seconds": runtime,
            "throughput_msg_per_sec": throughput,
            "avg_processing_time_ms": avg_time * 1000,
            "group_id": self.group_id,
            "topics": self.topics,
        }
