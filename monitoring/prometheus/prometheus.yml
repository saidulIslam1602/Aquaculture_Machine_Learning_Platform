# =============================================================================
# PROMETHEUS CONFIGURATION - AQUACULTURE PLATFORM MONITORING SYSTEM
# =============================================================================
#
# WHAT IS THIS FILE?
# This file configures Prometheus, the monitoring system that collects and
# stores performance data from all parts of the aquaculture platform.
# Think of it as setting up a "health monitoring dashboard" for the entire system.
#
# WHAT IS PROMETHEUS?
# Prometheus is like a "data collector" that:
# - Gathers performance metrics from all services (API, ML, database, etc.)
# - Stores this data in a time-series database
# - Provides alerts when something goes wrong
# - Enables creation of dashboards and graphs
# - Helps track system health and performance over time
#
# HOW PROMETHEUS WORKS:
# 1. "Scraping": Prometheus regularly visits each service to collect metrics
# 2. "Storage": Metrics are stored with timestamps for historical analysis
# 3. "Querying": Data can be queried to create graphs and alerts
# 4. "Alerting": Rules trigger notifications when thresholds are exceeded
#
# KEY CONCEPTS FOR BEGINNERS:
# - Metrics: Numerical data about system performance (CPU, memory, requests)
# - Scrape: Process of collecting metrics from services
# - Target: A service that Prometheus monitors
# - Job: A group of similar targets (all API servers, all databases)
# - Labels: Tags that help organize and filter metrics
#
# AUTHOR: DevOps Team
# VERSION: 1.0.0
# UPDATED: 2024-10-26
# =============================================================================

# =============================================================================
# GLOBAL CONFIGURATION - BASIC PROMETHEUS SETTINGS
# =============================================================================
# These settings apply to all monitoring activities

global:
  scrape_interval: 15s              # How often to collect metrics from services (every 15 seconds)
  evaluation_interval: 15s          # How often to check alerting rules (every 15 seconds)
  
  # External labels help identify this Prometheus instance in multi-cluster setups
  external_labels:
    cluster: 'aquaculture-ml-platform'     # Which cluster this monitors
    environment: 'production'              # Which environment (production, staging, dev)
    datacenter: 'primary'                  # Which datacenter or region

# =============================================================================
# ALERTING RULES - AUTOMATED PROBLEM DETECTION
# =============================================================================
# These files contain rules that automatically detect problems and send alerts

rule_files:
  - "rules/api_alerts.yml"              # API service performance and error alerts
  - "rules/infrastructure_alerts.yml"   # Server and infrastructure health alerts
  - "rules/business_alerts.yml"         # Business metric and KPI alerts
  - "rules/ml_model_alerts.yml"         # ML model performance and accuracy alerts
  - "rules/sla_alerts.yml"             # Service level agreement violation alerts
  - "rules/security_alerts.yml"        # Security incident and breach alerts

# =============================================================================
# ALERTMANAGER CONFIGURATION - NOTIFICATION SYSTEM
# =============================================================================
# Alertmanager receives alerts from Prometheus and sends notifications

alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - alertmanager:9093           # Alertmanager service location
      timeout: 10s                     # How long to wait for Alertmanager response
      api_version: v2                  # Alertmanager API version to use
      path_prefix: /                   # URL path prefix for Alertmanager
      scheme: http                     # Protocol to use (http or https)

# =============================================================================
# SCRAPE CONFIGURATIONS - WHAT SERVICES TO MONITOR
# =============================================================================
# Each job defines how Prometheus collects metrics from different services

scrape_configs:
  # =============================================================================
  # PROMETHEUS SELF-MONITORING
  # =============================================================================
  # Monitor Prometheus itself to ensure the monitoring system is healthy
  
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']    # Prometheus runs on port 9090
    scrape_interval: 15s               # Check every 15 seconds
    metrics_path: '/metrics'           # Where Prometheus exposes its own metrics
    honor_labels: true                 # Keep original metric labels

  # =============================================================================
  # AQUACULTURE API SERVICE MONITORING
  # =============================================================================
  # Monitor the main backend API that handles user requests
  
  - job_name: 'aquaculture-api'
    static_configs:
      - targets: ['api:8000']          # API service runs on port 8000
    metrics_path: '/metrics'           # API metrics endpoint
    scrape_interval: 5s                # Check every 5 seconds (more frequent for critical service)
    scrape_timeout: 10s                # Wait up to 10 seconds for response
    honor_labels: true                 # Preserve original labels from service
    params:
      format: ['prometheus']           # Request metrics in Prometheus format
    
    # Metric relabeling: Add component labels to organize metrics
    metric_relabel_configs:
      - source_labels: [__name__]      # Look at metric names
        regex: 'http_.*'               # If metric starts with "http_"
        target_label: component        # Add a "component" label
        replacement: 'api'             # Set component value to "api"
      - source_labels: [__name__]
        regex: 'fastapi_.*'            # If metric starts with "fastapi_"
        target_label: component
        replacement: 'api'

  # ML Service Monitoring
  - job_name: 'aquaculture-ml-service'
    static_configs:
      - targets: ['ml-service:8001']
    metrics_path: '/metrics'
    scrape_interval: 10s
    scrape_timeout: 15s
    metric_relabel_configs:
      - source_labels: [__name__]
        regex: 'model_.*'
        target_label: component
        replacement: 'ml'

  # Worker Service (Celery)
  - job_name: 'aquaculture-worker'
    static_configs:
      - targets: ['worker:9540']  # Celery exporter
    metrics_path: '/metrics'
    scrape_interval: 30s

  # PostgreSQL Database
  - job_name: 'postgres'
    static_configs:
      - targets: ['postgres-exporter:9187']
    scrape_interval: 30s
    scrape_timeout: 10s
    metric_relabel_configs:
      - source_labels: [__name__]
        regex: 'pg_.*'
        target_label: component
        replacement: 'database'

  # Redis Cache
  - job_name: 'redis'
    static_configs:
      - targets: ['redis-exporter:9121']
    scrape_interval: 30s
    metric_relabel_configs:
      - source_labels: [__name__]
        regex: 'redis_.*'
        target_label: component
        replacement: 'cache'

  # Kafka Message Broker
  - job_name: 'kafka'
    static_configs:
      - targets: ['kafka-exporter:9308']
    scrape_interval: 30s
    metric_relabel_configs:
      - source_labels: [__name__]
        regex: 'kafka_.*'
        target_label: component
        replacement: 'messaging'

  # Node Exporter (System Metrics)
  - job_name: 'node-exporter'
    static_configs:
      - targets: ['node-exporter:9100']
    scrape_interval: 15s
    metric_relabel_configs:
      - source_labels: [__name__]
        regex: 'node_.*'
        target_label: component
        replacement: 'infrastructure'

  # cAdvisor (Container Metrics)
  - job_name: 'cadvisor'
    static_configs:
      - targets: ['cadvisor:8080']
    scrape_interval: 15s
    metrics_path: '/metrics'
    metric_relabel_configs:
      - source_labels: [__name__]
        regex: 'container_.*'
        target_label: component
        replacement: 'containers'

  # Grafana Monitoring
  - job_name: 'grafana'
    static_configs:
      - targets: ['grafana:3000']
    metrics_path: '/metrics'
    scrape_interval: 30s

  # Jaeger Tracing
  - job_name: 'jaeger'
    static_configs:
      - targets: ['jaeger:14269']
    metrics_path: '/metrics'
    scrape_interval: 30s

  # NGINX Load Balancer
  - job_name: 'nginx'
    static_configs:
      - targets: ['nginx:9113']  # nginx-prometheus-exporter
    scrape_interval: 30s
    metric_relabel_configs:
      - source_labels: [__name__]
        regex: 'nginx_.*'
        target_label: component
        replacement: 'loadbalancer'

  # Custom Business Metrics
  - job_name: 'business-metrics'
    static_configs:
      - targets: ['api:8000']
    metrics_path: '/business-metrics'
    scrape_interval: 60s
    honor_labels: true

# Remote write configuration for long-term storage (optional)
# remote_write:
#   - url: "https://prometheus-remote-write-endpoint/api/v1/write"
#     queue_config:
#       max_samples_per_send: 1000
#       max_shards: 200
#       capacity: 2500

# Storage configuration is now handled via command line args in docker-compose.yml