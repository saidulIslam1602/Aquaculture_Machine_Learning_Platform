# =============================================================================
# PROMETHEUS ALERTING RULES - AQUACULTURE PLATFORM AUTOMATED NOTIFICATIONS
# =============================================================================
#
# WHAT IS THIS FILE?
# This file defines "alerting rules" that automatically send notifications
# when something goes wrong with the aquaculture platform. Think of it as
# setting up "smoke detectors" that alert the team when problems occur.
#
# WHAT ARE ALERTING RULES?
# Alerting rules are conditions that trigger notifications:
# - "If API response time > 5 seconds for 2 minutes → Send alert"
# - "If database CPU > 90% for 5 minutes → Send critical alert"
# - "If ML service is down → Send immediate notification"
#
# WHY USE AUTOMATED ALERTS?
# Instead of manually checking dashboards 24/7, alerts notify the team when:
# - Services go down (so users can't access the platform)
# - Performance degrades (slow responses, high error rates)
# - Resources are running low (disk space, memory)
# - Business metrics are abnormal (no predictions being made)
#
# ALERT SEVERITY LEVELS:
# - CRITICAL: Immediate action required (service down, data loss risk)
# - WARNING: Attention needed soon (performance degradation)
# - INFO: Awareness alerts (maintenance windows, deployments)
#
# ALERT CATEGORIES:
# - Infrastructure: Server health, network, storage
# - Application: Service availability, errors, performance
# - Business: Fish farm metrics, prediction accuracy, user activity
# - Security: Failed logins, suspicious activity
#
# HOW ALERTS WORK:
# 1. Prometheus evaluates these rules every 30 seconds
# 2. When a condition is met, an alert is "fired"
# 3. Alertmanager receives the alert and sends notifications
# 4. Notifications go to Slack, email, or on-call systems
# 5. Team investigates and resolves the issue
#
# AUTHOR: DevOps Team
# VERSION: 1.0.0
# UPDATED: 2024-10-26
# =============================================================================

# =============================================================================
# PROMETHEUS RULE RESOURCE DEFINITION
# =============================================================================

apiVersion: monitoring.coreos.com/v1        # Prometheus Operator API version
kind: PrometheusRule                        # Resource type: PrometheusRule
metadata:
  name: aquaculture-platform-alerts        # Rule set name
  labels:
    prometheus: kube-prometheus             # Prometheus instance to use
    role: alert-rules                       # Role identifier
    app: aquaculture-platform              # Application identifier

spec:
  # =============================================================================
  # ALERT RULE GROUPS - ORGANIZED BY COMPONENT AND SEVERITY
  # =============================================================================
  
  groups:
  
  # =============================================================================
  # CRITICAL INFRASTRUCTURE ALERTS - IMMEDIATE ATTENTION REQUIRED
  # =============================================================================
  # These alerts indicate serious problems that need immediate response
  
  - name: infrastructure.critical           # Rule group name
    interval: 30s                          # How often to evaluate rules
    rules:
    
    # Node Down Alert - Server/Worker Node Failure
    # This alert fires when a Kubernetes node (server) stops responding
    - alert: NodeDown
      expr: up{job="node-exporter"} == 0   # Condition: node-exporter is down
      for: 1m                              # Wait 1 minute before alerting (avoid false positives)
      labels:
        severity: critical                  # Alert severity level
        component: infrastructure           # Which component is affected
        team: sre                          # Which team should respond
      annotations:
        summary: "Node is down"                                                    # Short description
        description: "Node {{ $labels.instance }} has been down for more than 1 minute"  # Detailed description
        runbook_url: "https://docs.aquaculture.com/runbooks/node-down"           # Link to troubleshooting guide
    
    - alert: NodeMemoryHigh
      expr: |
        (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / 
        node_memory_MemTotal_bytes > 0.9
      for: 5m
      labels:
        severity: critical
        component: infrastructure
        team: sre
      annotations:
        summary: "Node memory usage critical"
        description: "Node {{ $labels.instance }} memory usage is {{ $value | humanizePercentage }}"

    - alert: NodeDiskSpaceCritical
      expr: |
        (node_filesystem_size_bytes{fstype!="tmpfs"} - node_filesystem_free_bytes{fstype!="tmpfs"}) /
        node_filesystem_size_bytes{fstype!="tmpfs"} > 0.95
      for: 2m
      labels:
        severity: critical
        component: infrastructure
        team: sre
      annotations:
        summary: "Node disk space critical"
        description: "Node {{ $labels.instance }} disk usage is {{ $value | humanizePercentage }}"

  # ========================================
  # APPLICATION SERVICE ALERTS
  # ========================================
  - name: application.critical
    interval: 30s
    rules:
    
    - alert: ServiceDown
      expr: up{job=~"api-service|ml-service|worker-service"} == 0
      for: 1m
      labels:
        severity: critical
        component: application
        team: backend
      annotations:
        summary: "Service is down"
        description: "Service {{ $labels.job }} instance {{ $labels.instance }} is down"
        runbook_url: "https://docs.aquaculture.com/runbooks/service-down"
    
    - alert: HighErrorRate
      expr: |
        rate(http_requests_total{status=~"5.."}[5m]) /
        rate(http_requests_total[5m]) > 0.1
      for: 3m
      labels:
        severity: critical
        component: application
        team: backend
      annotations:
        summary: "High error rate detected"
        description: "Error rate for {{ $labels.job }} is {{ $value | humanizePercentage }}"
    
    - alert: HighResponseTime
      expr: |
        histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 2.0
      for: 5m
      labels:
        severity: warning
        component: application
        team: backend
      annotations:
        summary: "High response time"
        description: "95th percentile response time for {{ $labels.job }} is {{ $value }}s"

  # ========================================
  # DATABASE ALERTS
  # ========================================
  - name: database.critical
    interval: 60s
    rules:
    
    - alert: PostgreSQLDown
      expr: pg_up == 0
      for: 1m
      labels:
        severity: critical
        component: database
        team: dba
      annotations:
        summary: "PostgreSQL is down"
        description: "PostgreSQL instance {{ $labels.instance }} is down"
        runbook_url: "https://docs.aquaculture.com/runbooks/postgresql-down"
    
    - alert: PostgreSQLConnectionsHigh
      expr: |
        pg_stat_database_numbackends / pg_settings_max_connections > 0.8
      for: 5m
      labels:
        severity: warning
        component: database
        team: dba
      annotations:
        summary: "PostgreSQL connections high"
        description: "PostgreSQL connection usage is {{ $value | humanizePercentage }}"
    
    - alert: PostgreSQLSlowQueries
      expr: |
        rate(pg_stat_database_blk_read_time[10m]) / rate(pg_stat_database_blks_read[10m]) > 0.01
      for: 10m
      labels:
        severity: warning
        component: database
        team: dba
      annotations:
        summary: "PostgreSQL slow queries detected"
        description: "Average query time is {{ $value }}s for database {{ $labels.datname }}"

  # ========================================
  # REDIS CACHE ALERTS
  # ========================================
  - name: redis.critical
    interval: 30s
    rules:
    
    - alert: RedisDown
      expr: redis_up == 0
      for: 1m
      labels:
        severity: critical
        component: cache
        team: backend
      annotations:
        summary: "Redis is down"
        description: "Redis instance {{ $labels.instance }} is down"
        runbook_url: "https://docs.aquaculture.com/runbooks/redis-down"
    
    - alert: RedisMemoryHigh
      expr: |
        redis_memory_used_bytes / redis_memory_max_bytes > 0.9
      for: 5m
      labels:
        severity: warning
        component: cache
        team: backend
      annotations:
        summary: "Redis memory usage high"
        description: "Redis memory usage is {{ $value | humanizePercentage }}"
    
    - alert: RedisConnectionsHigh
      expr: |
        redis_connected_clients / redis_config_maxclients > 0.8
      for: 5m
      labels:
        severity: warning
        component: cache
        team: backend
      annotations:
        summary: "Redis connections high"
        description: "Redis connection usage is {{ $value | humanizePercentage }}"

  # ========================================
  # ML SERVICE ALERTS
  # ========================================
  - name: ml-service.critical
    interval: 60s
    rules:
    
    - alert: MLModelInferenceFailureHigh
      expr: |
        rate(ml_inference_failures_total[10m]) /
        rate(ml_inference_requests_total[10m]) > 0.05
      for: 5m
      labels:
        severity: critical
        component: ml
        team: ml-ops
      annotations:
        summary: "ML model inference failure rate high"
        description: "ML inference failure rate is {{ $value | humanizePercentage }}"
    
    - alert: MLModelLatencyHigh
      expr: |
        histogram_quantile(0.95, rate(ml_inference_duration_seconds_bucket[10m])) > 5.0
      for: 5m
      labels:
        severity: warning
        component: ml
        team: ml-ops
      annotations:
        summary: "ML model inference latency high"
        description: "95th percentile ML inference latency is {{ $value }}s"
    
    - alert: MLModelAccuracyDrop
      expr: |
        ml_model_accuracy < 0.85
      for: 15m
      labels:
        severity: warning
        component: ml
        team: ml-ops
      annotations:
        summary: "ML model accuracy below threshold"
        description: "Model {{ $labels.model_name }} accuracy is {{ $value | humanizePercentage }}"

  # ========================================
  # WORKER SERVICE ALERTS
  # ========================================
  - name: worker.critical
    interval: 60s
    rules:
    
    - alert: CeleryQueueBacklog
      expr: |
        celery_queue_length > 1000
      for: 5m
      labels:
        severity: warning
        component: worker
        team: backend
      annotations:
        summary: "Celery queue backlog high"
        description: "Queue {{ $labels.queue }} has {{ $value }} pending tasks"
    
    - alert: CeleryWorkerDown
      expr: |
        celery_worker_up == 0
      for: 2m
      labels:
        severity: critical
        component: worker
        team: backend
      annotations:
        summary: "Celery worker is down"
        description: "Worker {{ $labels.worker }} is not responding"
    
    - alert: HighTaskFailureRate
      expr: |
        rate(celery_task_failures_total[10m]) /
        rate(celery_task_total[10m]) > 0.1
      for: 5m
      labels:
        severity: warning
        component: worker
        team: backend
      annotations:
        summary: "High task failure rate"
        description: "Task failure rate is {{ $value | humanizePercentage }}"

  # ========================================
  # BUSINESS METRIC ALERTS
  # ========================================
  - name: business.critical
    interval: 300s
    rules:
    
    - alert: DataIngestionStopped
      expr: |
        increase(business_data_ingestion_total[1h]) == 0
      for: 30m
      labels:
        severity: critical
        component: business
        team: data
      annotations:
        summary: "Data ingestion has stopped"
        description: "No data has been ingested in the last hour"
    
    - alert: PredictionAccuracyDegraded
      expr: |
        business_prediction_accuracy < 0.8
      for: 1h
      labels:
        severity: warning
        component: business
        team: ml-ops
      annotations:
        summary: "Prediction accuracy degraded"
        description: "Overall prediction accuracy is {{ $value | humanizePercentage }}"
    
    - alert: ActiveUsersDropped
      expr: |
        business_active_users < 100
      for: 15m
      labels:
        severity: warning
        component: business
        team: product
      annotations:
        summary: "Active users count dropped"
        description: "Active users count is {{ $value }}"

  # ========================================
  # SECURITY ALERTS
  # ========================================
  - name: security.critical
    interval: 60s
    rules:
    
    - alert: HighAuthenticationFailures
      expr: |
        rate(http_requests_total{status="401"}[5m]) > 10
      for: 2m
      labels:
        severity: warning
        component: security
        team: security
      annotations:
        summary: "High authentication failure rate"
        description: "Authentication failure rate is {{ $value }} per second"
    
    - alert: SuspiciousAPIActivity
      expr: |
        rate(http_requests_total[5m]) > 1000
      for: 5m
      labels:
        severity: warning
        component: security
        team: security
      annotations:
        summary: "Suspicious API activity detected"
        description: "Request rate is {{ $value }} per second from {{ $labels.instance }}"